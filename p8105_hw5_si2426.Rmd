---
title: "Homework 5"
output: github_document
date: "2022-11-03"
---

```{r, include = FALSE}
library(tidyverse)
library(ggplot2)

set.seed(1919)
```

## Problem 2
```{r}
homicide_df = read.csv("./data/homicide-data.csv")
```
The raw data contains `r nrow(homicide_df)` observations and `r ncol(homicide_df)` variables. The key variables are uid, reported date, victim last name, victim first name, victim race, victim age, victim sex, city, state, latitude, longitude, and disposition. The dataset contains information from `r homicide_df %>% distinct(state) %>% nrow()` distinct states.

```{r}
homicide_df = homicide_df %>% 
  mutate(city_state = paste(city, state, sep = ", "))

homicide_summary = homicide_df %>% 
  group_by(city_state) %>% 
  summarize(total_homicide = n(), unsolved_homicides = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest"))

head(homicide_summary, 5) %>% 
  knitr::kable()
```

```{r}
baltimoreptest = 
  prop.test(homicide_summary %>% 
                             filter(city_state == "Baltimore, MD") %>%
                             pull(unsolved_homicides), homicide_summary %>%
                             filter(city_state == "Baltimore, MD") %>%
                             pull(total_homicide))

baltimoreptest = baltimoreptest %>% 
  broom::tidy()
```

The estimated proportion is approximately `r round(baltimoreptest %>% select(estimate), 2)` and the 95% confidence interval is (`r round(baltimoreptest %>% select(conf.low, conf.high), 2)`).

```{r}
homicide_sum = homicide_summary %>% 
  mutate(
    prop_test = map2(
        homicide_summary$unsolved_homicides,
        homicide_summary$total_homicide, prop.test), 
    results = map(prop_test, broom::tidy)
    ) %>% 
  unnest(results) %>% 
  select(-c(prop_test, parameter, method, alternative)) %>%
  mutate(city_state = fct_reorder(city_state, estimate))

ggplot(homicide_sum, aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(x = "City, State", y = "Proportion of Unsolved Homicides") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

From the graph above, we note that Tulsa, Alabama has the lowest estimated proportion of unsolved homicides at 0 while Chicago, Illinois has the higheest estimated proportion proportion of unsolved homicides. It is important to note that Tulsa, Alabama only has `r nrow(homicide_df %>% filter(city_state == "Tulsa, AL"))` homicide, which was solved, meaning it has no unsolved homicides in this dataset.

## Problem 3

#### Simulating $\mu$ = 0
```{r}
sim = function(n = 30, mu, sigma = 5) 
  {
  sim_data = tibble(
    x = rnorm(n = n, mean = mu, sd = sigma),
  )
  }

mu_0_df = 
  expand_grid(
    sample_size = 30,
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(sample_size, ~sim(mu = 0))
  ) %>% 
  group_by(iter) %>%
  mutate(
    mean = map_dbl(.x = estimate_df, ~mean(.x$x)),
    t_test = map(.x = estimate_df, ~t.test(.x$x)),
    results = map(t_test, broom::tidy)
    ) 

clean_t_test = function(x) 
  {
   x %>% unnest(results) %>% 
  dplyr::select(
    -c("estimate_df", "t_test", "parameter", "method", "alternative", "estimate")) %>%
    mutate(null_rejected = as.logical(ifelse(p.value > 0.05, 0, 1)))
  }

mu_0_df = mu_0_df %>% clean_t_test

head(mu_0_df, 5)
```

#### Simulating Different for Different $\mu$
```{r}
combined_mu_df = vector("list", 6)

bind = for(i in 1:6)
{
  combined_mu_df[[i]] = (expand_grid(
    sample_size = 30,
    iter = 1:100
  ) %>% 
  mutate(
    estimate_df = map(sample_size, ~sim(mu = i))
  ) %>% 
  group_by(iter) %>%
  mutate(
    mu = i,
    mean = map_dbl(.x = estimate_df, ~mean(.x$x)),
    t_test = map(.x = estimate_df, ~t.test(.x$x)),
    results = map(t_test, broom::tidy)
    ) %>% 
    clean_t_test)
}

combined_mu_df = combined_mu_df %>% do.call(rbind, .)
```

#### Proportion of Times Null Rejected
```{r}
null_reject_df = combined_mu_df %>% 
  group_by(mu) %>% 
  summarize(null_rejected_freq = sum(null_rejected)/n())

ggplot(null_reject_df, aes(x = mu, y = null_rejected_freq)) + 
  geom_point() +
  labs(x = "True Value of Mu", y = "Proportion of Times Null Hypothesis Rejected (Power)") + 
  theme(text = element_text(size = 10))
```

The plot above indicates that the power of the test increases with the effect size. 

```{r}
mean_estimate_df = combined_mu_df %>% 
  group_by(mu) %>%
  summarize(mean_estimate = mean(mean))

rejected_mean_estimate_df = combined_mu_df %>% 
  filter(null_rejected == 1) %>%
  group_by(mu) %>%
  summarize(mean_estimate = mean(mean))

ggplot(mean_estimate_df, aes(x = mu, y = mean_estimate)) + 
  geom_point(aes(color = "b", shape = 'b', size = 'b')) + 
  geom_point(
    data = rejected_mean_estimate_df, aes(color = "a", shape = 'a', size = 'a')) +
  labs(x = "True Value of Mu", y = "Mean of Estimate of Mu", color = "Legend") +
    scale_color_manual(name = 'Type of Sample', 
                     values = c('b' = 'black','a' = 'red'), 
                     labels = c('All','Null Hypothesis Rejected')) + 
    scale_shape_manual(name = 'Type of Sample',
                       labels = c('All','Null Hypothesis Rejected'),
      values = c('b' = 19, 'a' = 17)) + 
      scale_size_manual(name = 'Type of Sample',
                       labels = c('All','Null Hypothesis Rejected'),
      values = c('b' = 3, 'a' = 2))

```

The red dots represent the average estimated values of $\hat{\mu}$ in samples for which the null was rejected. The black dots represent the average estimated values of $\hat{\mu}$ for all samples. The black dots are much more accurate to the true value of $\mu$. This is not surprising, as the samples where the null was rejected indicate that the estimated value of $\mu$ was very different from the true value.  
