P8105 Homework 5 si2426
================
Shodai Inose
2022-11-16

## Problem 2

#### Loading Data and Creating a Summary

``` r
homicide_df = read.csv("./data/homicide-data.csv") %>% 
transform(
  state,
  state = replace(state, (city == "Tulsa" & state == "AL"), "OK")
) %>%
  select(-"c..NM....NM....NM....NM....NM....NM....NM....NM....NM....NM...") %>%
  mutate(victim_age = as.integer(victim_age),
         victim_race = as.factor(victim_race),
         victim_sex = as.factor(victim_sex))
```

    ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion

The raw data contains 52179 observations and 12 variables. The key
variables are `uid`, `reported_date`, `victim_last`, `victim_first`,
`victim_race`, `victim_age`, `victim_sex`, `city`, `state`, `lat`,
`lon`, and `disposition.` The dataset contains information from 28
distinct states. The original data contains a likely typo of a city
named Tulsa, Alabama. As there are no cities or towns in Alabama with
the name Tulsa, this error has been corrected to Tulsa, Oklahoma.

``` r
homicide_df = homicide_df %>% 
  mutate(city_state = paste(city, state, sep = ", "))

homicide_summary = homicide_df %>% 
  group_by(city_state) %>% 
  summarize(total_homicide = n(), unsolved_homicides = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest"))

head(homicide_summary, 5) %>% 
  knitr::kable()
```

| city_state      | total_homicide | unsolved_homicides |
|:----------------|---------------:|-------------------:|
| Albuquerque, NM |            378 |                146 |
| Atlanta, GA     |            973 |                373 |
| Baltimore, MD   |           2827 |               1825 |
| Baton Rouge, LA |            424 |                196 |
| Birmingham, AL  |            800 |                347 |

#### Proportion of Unsolved Homicides in Baltimore, MD

``` r
baltimoreptest = 
  prop.test(homicide_summary %>% 
                             filter(city_state == "Baltimore, MD") %>%
                             pull(unsolved_homicides), homicide_summary %>%
                             filter(city_state == "Baltimore, MD") %>%
                             pull(total_homicide))

baltimoreptest = baltimoreptest %>% 
  broom::tidy()

baltimoreptest_estimate = round(baltimoreptest %>% pull(estimate), 2)

baltimoreptest_low = round(baltimoreptest %>% pull(conf.low), 2)
baltimoreptest_high = round(baltimoreptest %>% pull(conf.high), 2)
```

The estimated proportion is approximately 0.65 and the 95% confidence
interval is (0.63, 0.66).

#### Proportion of Unsolved Homicides in All Cities

``` r
homicide_sum = homicide_summary %>% 
  mutate(
    prop_test = map2(
        homicide_summary$unsolved_homicides,
        homicide_summary$total_homicide, prop.test), 
    results = map(prop_test, broom::tidy)
    ) %>% 
  unnest(results) %>% 
  select(-c(prop_test, parameter, method, alternative)) %>%
  mutate(city_state = fct_reorder(city_state, estimate))

head(homicide_sum)
```

    ## # A tibble: 6 × 8
    ##   city_state      total_homic…¹ unsol…² estim…³ stati…⁴  p.value conf.…⁵ conf.…⁶
    ##   <fct>                   <int>   <int>   <dbl>   <dbl>    <dbl>   <dbl>   <dbl>
    ## 1 Albuquerque, NM           378     146   0.386 1.91e+1 1.23e- 5   0.337   0.438
    ## 2 Atlanta, GA               973     373   0.383 5.25e+1 4.32e-13   0.353   0.415
    ## 3 Baltimore, MD            2827    1825   0.646 2.39e+2 6.46e-54   0.628   0.663
    ## 4 Baton Rouge, LA           424     196   0.462 2.27e+0 1.32e- 1   0.414   0.511
    ## 5 Birmingham, AL            800     347   0.434 1.38e+1 2.05e- 4   0.399   0.469
    ## 6 Boston, MA                614     310   0.505 4.07e-2 8.40e- 1   0.465   0.545
    ## # … with abbreviated variable names ¹​total_homicide, ²​unsolved_homicides,
    ## #   ³​estimate, ⁴​statistic, ⁵​conf.low, ⁶​conf.high

#### Plotting the Proportions of Unsolved Homicides

``` r
ggplot(homicide_sum, aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(x = "City, State", y = "Proportion of Unsolved Homicides") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

![](p8105_hw5_si2426_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

From the graph above, we note that Richmond, Virginia has the lowest
estimated proportion of unsolved homicides while Chicago, Illinois has
the highest estimated proportion proportion of unsolved homicides. It
should be noted that the confidence interval for the proportion of
unsolved homicides in Chicago is much higher than the confidence
intervals in other cities. Since the confidence interval for Chicago
does not overlap with any other confidence intervals, it indicates a
significant difference between the proportion of unsolved homicides in
Chicago and all other cities.

## Problem 3

#### Simulating $\mu$ = 0

``` r
simulation = function(n = 30, mu, sigma = 5) 
  {
  sim_data = tibble(
    x = rnorm(n = n, mean = mu, sd = sigma),
  )
  }

mu_0_df = 
  expand_grid(
    sample_size = 30,
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(sample_size, ~simulation(mu = 0))
  ) %>% 
  group_by(iter) %>%
  mutate(
    mean = map_dbl(.x = estimate_df, ~mean(.x$x)),
    t_test = map(.x = estimate_df, ~t.test(.x$x)),
    results = map(t_test, broom::tidy)
    ) 

clean_t_test = function(x) 
  {
   x %>% unnest(results) %>% 
  dplyr::select(
    -c("estimate_df", "t_test", "parameter", "method", "alternative", "estimate")) %>%
    mutate(null_rejected = as.logical(ifelse(p.value > 0.05, 0, 1)))
  }

mu_0_df = mu_0_df %>% clean_t_test

head(mu_0_df, 5)
```

    ## # A tibble: 5 × 8
    ## # Groups:   iter [5]
    ##   sample_size  iter   mean statistic p.value conf.low conf.high null_rejected
    ##         <dbl> <int>  <dbl>     <dbl>   <dbl>    <dbl>     <dbl> <lgl>        
    ## 1          30     1  0.398     0.520   0.607    -1.17      1.96 FALSE        
    ## 2          30     2  0.175     0.241   0.811    -1.31      1.66 FALSE        
    ## 3          30     3 -0.298    -0.320   0.751    -2.20      1.60 FALSE        
    ## 4          30     4  0.277     0.285   0.778    -1.71      2.26 FALSE        
    ## 5          30     5 -0.534    -0.475   0.638    -2.83      1.76 FALSE

#### Simulating Different Values of $\mu$

``` r
combined_mu_df = vector("list", 6)

bind = for(i in 1:6)
{
  combined_mu_df[[i]] = (expand_grid(
    sample_size = 30,
    iter = 1:100
  ) %>% 
  mutate(
    estimate_df = map(sample_size, ~simulation(mu = i))
  ) %>% 
  group_by(iter) %>%
  mutate(
    mu = i,
    mean = map_dbl(.x = estimate_df, ~mean(.x$x)),
    t_test = map(.x = estimate_df, ~t.test(.x$x)),
    results = map(t_test, broom::tidy)
    ) %>% 
    clean_t_test)
}

combined_mu_df = combined_mu_df %>% do.call(rbind, .)
```

#### Proportion of Times Null Rejected

``` r
null_reject_df = combined_mu_df %>% 
  group_by(mu) %>% 
  summarize(null_rejected_freq = sum(null_rejected)/n())

ggplot(null_reject_df, aes(x = mu, y = null_rejected_freq)) + 
  geom_point() +
  labs(x = "True Value of Mu", y = "Proportion of Times Null Hypothesis Rejected (Power)") + 
  theme(text = element_text(size = 10))
```

![](p8105_hw5_si2426_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

The plot above indicates that the power of the test increases with the
effect size. For smaller effect sizes, the power quickly increases with
an increase in effect size. However, the relationship between effect
size and power slows as power approaches a value of 1. This occurs
because as the true value of $\mu$ increases further away from the
hypothesized value (which is 0 in this case), the null hypotheses of the
samples are more likely to be rejected.

``` r
mean_estimate_df = combined_mu_df %>% 
  group_by(mu) %>%
  summarize(mean_estimate = mean(mean))

rejected_mean_estimate_df = combined_mu_df %>% 
  filter(null_rejected == 1) %>%
  group_by(mu) %>%
  summarize(mean_estimate = mean(mean))

ggplot(mean_estimate_df, aes(x = mu, y = mean_estimate)) + 
  geom_point(aes(color = "b", shape = 'b', size = 'b')) + 
  geom_point(
    data = rejected_mean_estimate_df, aes(color = "a", shape = 'a', size = 'a')) +
  labs(x = "True Value of Mu", y = "Mean of Estimate of Mu", color = "Legend") +
    scale_color_manual(name = 'Type of Sample', 
                     values = c('b' = 'black','a' = 'red'), 
                     labels = c('All','Null Hypothesis Rejected')) + 
    scale_shape_manual(name = 'Type of Sample',
                       labels = c('All','Null Hypothesis Rejected'),
      values = c('b' = 19, 'a' = 17)) + 
      scale_size_manual(name = 'Type of Sample',
                       labels = c('All','Null Hypothesis Rejected'),
      values = c('b' = 3, 'a' = 2))
```

![](p8105_hw5_si2426_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

The red marks represent the average estimated values of $\hat{\mu}$ in
samples for which the null was rejected. For the smaller values of
$\hat{\mu}$, these differ greatly from the average estimated values of
$\hat{\mu}$ for all samples. The black dots represent the average
estimated values of $\hat{\mu}$ for all samples. The black dots are much
more accurate to the true value of $\mu$ than the red marks. This is not
surprising, as the samples where the null was rejected indicate that the
estimated value of $\mu$ was very different from the true value.
